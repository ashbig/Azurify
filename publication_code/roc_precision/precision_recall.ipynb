{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PrecisionRecallDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = pd.read_csv('order.txt', sep='\\t')\n",
    "order = order[['ExternalId', 'IndicationForStudy']]\n",
    "order['IndicationForStudy'] = order['IndicationForStudy'].str.strip()\n",
    "order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExternalId</th>\n",
       "      <th>PanelType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CPDC151546</td>\n",
       "      <td>HEME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CPDC151547</td>\n",
       "      <td>HEME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CPDC151548</td>\n",
       "      <td>HEME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CPDC151550</td>\n",
       "      <td>SOLID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CPDC151555</td>\n",
       "      <td>SOLID</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ExternalId PanelType\n",
       "0  CPDC151546      HEME\n",
       "1  CPDC151547      HEME\n",
       "2  CPDC151548      HEME\n",
       "3  CPDC151550     SOLID\n",
       "4  CPDC151555     SOLID"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panel = pd.read_csv('panel.tab', sep='\\t')\n",
    "panel = panel[['ExternalId', 'PanelType']]\n",
    "panel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adx = pd.merge(order, panel, on='ExternalId', how='inner')\n",
    "adx = adx[adx['PanelType'] == 'SOLID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adx.dropna(subset=['IndicationForStudy'], inplace=True)\n",
    "breast = adx[adx['IndicationForStudy'].str.contains('Breast', case=False)] #will capture all breast events regardless of EHR annotation\n",
    "breast.to_csv('adx_breast.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test50 = pd.read_csv('../data/test50.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcpdids = breast['ExternalId'].tolist()\n",
    "t50_breast = test50[test50['CPDID'].isin(bcpdids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the coutns of each diagnosis for solid panels\n",
    "vc = adx['IndicationForStudy'].value_counts()\n",
    "adx['DiagnosisCounts'] = adx['IndicationForStudy'].map(vc)\n",
    "sdx = adx[adx['DiagnosisCounts'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Use pd.qcut to assign quartile categories\n",
    "cdx = sdx.copy()\n",
    "cdx['Quartile'] = pd.qcut(sdx['DiagnosisCounts'], 4, labels=False)\n",
    "\n",
    "# Step 2: Filter the DataFrame into four separate DataFrames based on the quartile categories\n",
    "df_q1 = cdx[cdx['Quartile'] == 0]\n",
    "df_q2 = cdx[cdx['Quartile'] == 1]\n",
    "df_q3 = cdx[cdx['Quartile'] == 2]\n",
    "df_q4 = cdx[cdx['Quartile'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ash\\AppData\\Local\\Temp\\ipykernel_31456\\2710995269.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dxc.drop_duplicates(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dxc = cdx[['IndicationForStudy', 'DiagnosisCounts']]\n",
    "dxc.drop_duplicates(inplace=True)\n",
    "dxc.to_csv('SolidTumorTrainingDiagnosisCounts.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a dataframe for each quartile\n",
    "q1_ids = df_q1['ExternalId'].tolist()\n",
    "q2_ids = df_q2['ExternalId'].tolist()\n",
    "q3_ids = df_q3['ExternalId'].tolist()\n",
    "q4_ids = df_q4['ExternalId'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x29197dd4110>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compM = CatBoostClassifier()\n",
    "compM.load_model('../../models/azurify_hg19.0.99.json', format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data, ids, model):\n",
    "    data = data.sort_values('CATEGORIZATION') # sort the data ensure class order for plotting\n",
    "    data = data[data['CPDID'].isin(ids)]\n",
    "    truth = data['CATEGORIZATION']\n",
    "    ds = data[['PCHANGE', 'GENE', 'Domain', 'ClinicalSignificance','EFFECT', 'Civic_Evidence', 'FAF', 'GNOMAD_AC', 'GNOMAD_AF', 'EXON_Rank', 'COSMIC_CNT','MVP_score', 'Civic_Drug', 'PMID_COUNT', 'KEGG']]\n",
    "    \n",
    "    score = model.predict_proba(ds)\n",
    "\n",
    "    return score, truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1s, q1t = get_predictions(test50, q1_ids, compM)\n",
    "q2s,q2t = get_predictions(test50, q2_ids, compM)\n",
    "q3s, q3t = get_predictions(test50, q3_ids, compM)\n",
    "q4s, q4t = get_predictions(test50, q4_ids, compM)\n",
    "bs, bt = get_predictions(t50_breast, bcpdids, compM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pr_curve(y_score, y_test, title):\n",
    "    n_classes = 5\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    classes = y_test.unique()\n",
    "    y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "    plt.rcParams['pdf.fonttype'] = 42\n",
    "    plt.rcParams['ps.fonttype'] = 42\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "\n",
    "    # A \"micro-average\": quantifying score on all classes jointly\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n",
    "        y_test_bin.ravel(), y_score.ravel()\n",
    "    )\n",
    "    average_precision[\"micro\"] = average_precision_score(y_test_bin, y_score, average=\"micro\")\n",
    "\n",
    "    # setup plot details\n",
    "    colors = cycle([\"#282A74\", \"#991B1E\", \"#6CA9DC\", \"#FFCE06\", \"#8A8D8F\"])\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines, labels = [], []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "        plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[\"micro\"],\n",
    "        precision=precision[\"micro\"],\n",
    "        average_precision=average_precision[\"micro\"],\n",
    "    )\n",
    "    display.plot(ax=ax, name=\"Micro-average\", color=\"green\")\n",
    "\n",
    "    nl = ['Benign', 'Pathogenic', 'Likely Benign', 'Likely Pathogenic', 'VUS']\n",
    "\n",
    "    for i, color in zip(range(len(classes)), colors):\n",
    "        display = PrecisionRecallDisplay(\n",
    "            recall=recall[i],\n",
    "            precision=precision[i],\n",
    "            average_precision=average_precision[i],\n",
    "        )\n",
    "        display.plot(ax=ax, name=f\"{nl[i]}\", color=color)\n",
    "\n",
    "    # add the legend for the iso-f1 curves\n",
    "    handles, labels = display.ax_.get_legend_handles_labels()\n",
    "    handles.extend([l])\n",
    "    labels.extend([\"iso-f1 curves\"])\n",
    "    # set the legend and the axes\n",
    "    ax.legend(handles=handles, labels=labels, loc=\"lower left\")\n",
    "    ax.set_title(title)\n",
    "    plt.savefig(title +'.pdf', dpi=600)\n",
    "    plt.clf()\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pr_curve(q1s, q1t, \"Q1 Diagnoses Frequency Precision-Recall\")\n",
    "make_pr_curve(q2s, q2t, \"Q2 Diagnoses Frequency Precision-Recall\")\n",
    "make_pr_curve(q3s, q3t, \"Q3 Diagnoses Frequency Precision-Recall\")\n",
    "make_pr_curve(q4s, q4t, \"Q4 Diagnoses Frequency Precision-Recall\")\n",
    "make_pr_curve(bs, bt, \"Breast Cancer Precision-Recal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total test set PR curve and an individual example\n",
    "ts = test50.sort_values('CATEGORIZATION') # sort the data ensure class order for plotting\n",
    "y_test = ts['CATEGORIZATION']\n",
    "s = ts[['PCHANGE', 'GENE', 'Domain', 'ClinicalSignificance','EFFECT', 'Civic_Evidence', 'FAF', 'GNOMAD_AC', 'GNOMAD_AF', 'EXON_Rank', 'COSMIC_CNT','MVP_score', 'Civic_Drug', 'PMID_COUNT', 'KEGG']]\n",
    "y_score = compM.predict_proba(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each class\n",
    "n_classes = 5\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "y_test_bin = label_binarize(y_test, classes=(y_test.unique()))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n",
    "    y_test_bin.ravel(), y_score.ravel()\n",
    ")\n",
    "average_precision[\"micro\"] = average_precision_score(y_test_bin, y_score, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# setup plot details\n",
    "colors = cycle([\"#282A74\", \"#991B1E\", \"#6CA9DC\", \"#FFCE06\", \"#8A8D8F\"])\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines, labels = [], []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "    plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "display = PrecisionRecallDisplay(\n",
    "    recall=recall[\"micro\"],\n",
    "    precision=precision[\"micro\"],\n",
    "    average_precision=average_precision[\"micro\"],\n",
    ")\n",
    "display.plot(ax=ax, name=\"Micro-average\", color=\"green\")\n",
    "\n",
    "nl = ['Benign', 'Pathogenic', 'Likely Benign', 'Likely Pathogenic', 'VUS']\n",
    "\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[i],\n",
    "        precision=precision[i],\n",
    "        average_precision=average_precision[i],\n",
    "    )\n",
    "    display.plot(ax=ax, name=f\"{nl[i]}\", color=color)\n",
    "\n",
    "# add the legend for the iso-f1 curves\n",
    "handles, labels = display.ax_.get_legend_handles_labels()\n",
    "handles.extend([l])\n",
    "labels.extend([\"iso-f1 curves\"])\n",
    "# set the legend and the axes\n",
    "ax.legend(handles=handles, labels=labels, loc=\"lower left\")\n",
    "ax.set_title(\"Test Data Precision-Recall\")\n",
    "plt.savefig(\"Test Data Precision Recall.pdf\", dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "y_pred = compM.predict(s)\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "# printing metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "az_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
